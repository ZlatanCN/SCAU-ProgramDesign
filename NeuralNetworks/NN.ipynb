{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc4ba7f2-e3be-460a-bac0-d6f5650f25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc99b43b-3dbf-4b77-92a9-2deb59b53471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Cat label is 3\n",
    "cat_train = X_train[y_train.flatten() == 3][:200]\n",
    "cat_test = X_test[y_test.flatten() == 3][:50]\n",
    "\n",
    "# Not-cat label is 0\n",
    "not_cat_train = X_train[y_train.flatten() == 0][:200]\n",
    "not_cat_test = X_test[y_test.flatten() == 0][:50]\n",
    "\n",
    "# Create cat and not-cat labels\n",
    "y_cat_train = np.ones((len(cat_train), 1))\n",
    "y_cat_test = np.ones((len(cat_test), 1))\n",
    "y_not_cat_train = np.zeros((len(not_cat_train), 1))\n",
    "y_not_cat_test = np.zeros((len(not_cat_test), 1))\n",
    "\n",
    "# Flatten image data\n",
    "cat_train_flat = cat_train.reshape(len(cat_train), -1).T\n",
    "cat_test_flat = cat_test.reshape(len(cat_test), -1).T\n",
    "not_cat_train_flat = not_cat_train.reshape(len(not_cat_train), -1).T\n",
    "not_cat_test_flat = not_cat_test.reshape(len(not_cat_test), -1).T\n",
    "\n",
    "# Combine cat and not-cat data\n",
    "X_train = np.hstack([cat_train_flat, not_cat_train_flat]) / 255\n",
    "X_test = np.hstack([cat_test_flat, not_cat_test_flat]) / 255\n",
    "\n",
    "# Combine cat and not-cat labels\n",
    "y_train = np.vstack([y_cat_train, y_not_cat_train]).T\n",
    "y_test = np.vstack([y_cat_test, y_not_cat_test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6fcfd1b3-6a43-450c-a679-c85b4b63a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3687)\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def LeakyReLU(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def LeakyReLU_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "# Initialize weights and biases\n",
    "def initialize_parameters(input_size, hidden_size1, hidden_size2, output_size):\n",
    "    # np.random.seed(42)  # For reproducibility\n",
    "    W1 = np.random.randn(hidden_size1, input_size) * 0.01\n",
    "    b1 = np.zeros((hidden_size1, 1))\n",
    "    W2 = np.random.randn(hidden_size2, hidden_size1) * 0.01\n",
    "    b2 = np.zeros((hidden_size2, 1))\n",
    "    W3 = np.random.randn(output_size, hidden_size2) * 0.01\n",
    "    b3 = np.zeros((output_size, 1))\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, W1, b1, W2, b2, W3, b3, active):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = active(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = active(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = active(Z3)\n",
    "    return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "# Compute cost\n",
    "def compute_cost(A3, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1 / m) * np.sum(Y * np.log(A3) + (1 - Y) * np.log(1 - A3))\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, Y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, active_derivative):\n",
    "    m = X.shape[1]\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1 / m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1 / m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = dA2 * active_derivative(Z2)\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * active_derivative(Z1)\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "# Update parameters\n",
    "def update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate, delay, iteration,num_iters):\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3\n",
    "    if iteration % 100 == 0:\n",
    "        learning_rate = learning_rate - learning_rate * delay * iteration / num_iters\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "# Neural network model\n",
    "def neural_network(X_train, Y_train, input_size, hidden_size1, hidden_size2, output_size, num_iterations,\n",
    "                   learning_rate):\n",
    "    W1, b1, W2, b2, W3, b3 = initialize_parameters(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X_train, W1, b1, W2, b2, W3, b3, sigmoid)\n",
    "        cost = compute_cost(A3, Y_train)\n",
    "\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backward_propagation(X_train, Y_train, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, sigmoid_derivative)\n",
    "        W1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate,delay,i,num_iterations)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "# Prediction\n",
    "def predict(X, W1, b1, W2, b2, W3, b3):\n",
    "    _, _, _, _, _, A3 = forward_propagation(X, W1, b1, W2, b2, W3, b3, sigmoid)\n",
    "    predictions = (A3 > 0.5).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a77f158a-a15f-4e5a-96e1-f72c47b19c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6936167203972181\n",
      "Cost after iteration 100: 0.6931389355013087\n",
      "Cost after iteration 200: 0.6931135340281063\n",
      "Cost after iteration 300: 0.6929948349644882\n",
      "Cost after iteration 400: 0.6917750944552219\n",
      "Cost after iteration 500: 0.5741419669712304\n",
      "Cost after iteration 600: 0.47046957710980536\n",
      "Cost after iteration 700: 0.42997557091570293\n",
      "Cost after iteration 800: 0.3426597298723258\n",
      "Cost after iteration 900: 0.30746405549139794\n",
      "Cost after iteration 1000: 0.2425757850740263\n",
      "Cost after iteration 1100: 0.2040464248354128\n",
      "Cost after iteration 1200: 0.13308052949863164\n",
      "Cost after iteration 1300: 0.3339817642701554\n",
      "Cost after iteration 1400: 0.2070946953998528\n",
      "Cost after iteration 1500: 0.1613179438702295\n",
      "Cost after iteration 1600: 0.44066219260053174\n",
      "Cost after iteration 1700: 0.10556499157923653\n",
      "Cost after iteration 1800: 0.036224823703766375\n",
      "Cost after iteration 1900: 0.019556176728426446\n",
      "Cost after iteration 2000: 0.01213692963596086\n",
      "Cost after iteration 2100: 0.008738448263829017\n",
      "Cost after iteration 2200: 0.006378901024130305\n",
      "Cost after iteration 2300: 0.005005513791730194\n",
      "Cost after iteration 2400: 0.0041002280507319066\n",
      "Cost after iteration 2500: 0.0034506983972036327\n",
      "Cost after iteration 2600: 0.002958085543864694\n",
      "Cost after iteration 2700: 0.002569044113570098\n",
      "Cost after iteration 2800: 0.0022542391507389637\n",
      "Cost after iteration 2900: 0.0019973988972303396\n",
      "Cost after iteration 3000: 0.0017866134878410356\n",
      "Cost after iteration 3100: 0.0016114269954167332\n",
      "Cost after iteration 3200: 0.0014633502301869895\n",
      "Cost after iteration 3300: 0.0013360374112764107\n",
      "Cost after iteration 3400: 0.0012250181290676945\n",
      "Cost after iteration 3500: 0.0011273262063877655\n",
      "Cost after iteration 3600: 0.0010409698839839287\n",
      "Cost after iteration 3700: 0.0009644032290957338\n",
      "Cost after iteration 3800: 0.0008963366939428319\n",
      "Cost after iteration 3900: 0.0008358454353061082\n",
      "Cost after iteration 4000: 0.0007823431941907702\n",
      "Cost after iteration 4100: 0.000735231303565168\n",
      "Cost after iteration 4200: 0.0006936909168547262\n",
      "Cost after iteration 4300: 0.0006568345726671518\n",
      "Cost after iteration 4400: 0.0006238815774180862\n",
      "Cost after iteration 4500: 0.0005942031969893149\n",
      "Cost after iteration 4600: 0.0005673021008840249\n",
      "Cost after iteration 4700: 0.0005427817515364566\n",
      "Cost after iteration 4800: 0.0005203215949595101\n",
      "Cost after iteration 4900: 0.0004996590397072916\n",
      "Cost after iteration 5000: 0.0004805764605344175\n",
      "Cost after iteration 5100: 0.0004628916520350509\n",
      "Cost after iteration 5200: 0.0004464506705780762\n",
      "Cost after iteration 5300: 0.0004311223748478829\n",
      "Cost after iteration 5400: 0.0004167942053549227\n",
      "Cost after iteration 5500: 0.00040336888495281137\n",
      "Cost after iteration 5600: 0.0003907618135056035\n",
      "Cost after iteration 5700: 0.0003788989913804896\n",
      "Cost after iteration 5800: 0.00036771534958831344\n",
      "Cost after iteration 5900: 0.0003571533953787173\n",
      "Cost after iteration 6000: 0.0003471621046647075\n",
      "Cost after iteration 6100: 0.00033769600923377446\n",
      "Cost after iteration 6200: 0.00032871443897124013\n",
      "Cost after iteration 6300: 0.0003201808884535362\n",
      "Cost after iteration 6400: 0.00031206248411228986\n",
      "Cost after iteration 6500: 0.00030432953333384003\n",
      "Cost after iteration 6600: 0.000296955140784437\n",
      "Cost after iteration 6700: 0.0002899148802594295\n",
      "Cost after iteration 6800: 0.0002831865126773774\n",
      "Cost after iteration 6900: 0.00027674974264806784\n",
      "Cost after iteration 7000: 0.000270586007461403\n",
      "Cost after iteration 7100: 0.0002646782934645056\n",
      "Cost after iteration 7200: 0.0002590109756863021\n",
      "Cost after iteration 7300: 0.0002535696772829531\n",
      "Cost after iteration 7400: 0.00024834114595406146\n",
      "Cost after iteration 7500: 0.00024331314494649127\n",
      "Cost after iteration 7600: 0.00023847435664464775\n",
      "Cost after iteration 7700: 0.00023381429705838814\n",
      "Cost after iteration 7800: 0.00022932323977822313\n",
      "Cost after iteration 7900: 0.00022499214818104285\n",
      "Cost after iteration 8000: 0.00022081261484769373\n",
      "Cost after iteration 8100: 0.00021677680730252874\n",
      "Cost after iteration 8200: 0.00021287741930984085\n",
      "Cost after iteration 8300: 0.00020910762706764202\n",
      "Cost after iteration 8400: 0.00020546104972796826\n",
      "Cost after iteration 8500: 0.00020193171374881607\n",
      "Cost after iteration 8600: 0.00019851402064685782\n",
      "Cost after iteration 8700: 0.00019520271777543268\n",
      "Cost after iteration 8800: 0.00019199287179926124\n",
      "Cost after iteration 8900: 0.00018887984457796687\n",
      "Cost after iteration 9000: 0.00018585927120530748\n",
      "Cost after iteration 9100: 0.00018292703998148245\n",
      "Cost after iteration 9200: 0.0001800792741218023\n",
      "Cost after iteration 9300: 0.00017731231502798972\n",
      "Cost after iteration 9400: 0.00017462270696797874\n",
      "Cost after iteration 9500: 0.00017200718302766535\n",
      "Cost after iteration 9600: 0.00016946265221289976\n",
      "Cost after iteration 9700: 0.00016698618759362682\n",
      "Cost after iteration 9800: 0.00016457501539342287\n",
      "Cost after iteration 9900: 0.0001622265049382999\n",
      "Cost after iteration 10000: 0.0001599381593874892\n",
      "Cost after iteration 10100: 0.0001577076071770813\n",
      "Cost after iteration 10200: 0.00015553259411443026\n",
      "Cost after iteration 10300: 0.00015341097606779013\n",
      "Cost after iteration 10400: 0.0001513407122008794\n",
      "Cost after iteration 10500: 0.00014931985870757835\n",
      "Cost after iteration 10600: 0.00014734656300607855\n",
      "Cost after iteration 10700: 0.00014541905835605852\n",
      "Cost after iteration 10800: 0.00014353565886592428\n",
      "Cost after iteration 10900: 0.00014169475486060817\n",
      "Cost after iteration 11000: 0.00013989480858304717\n",
      "Cost after iteration 11100: 0.0001381343502055141\n",
      "Cost after iteration 11200: 0.00013641197412895216\n",
      "Cost after iteration 11300: 0.00013472633555088213\n",
      "Cost after iteration 11400: 0.00013307614728447494\n",
      "Cost after iteration 11500: 0.00013146017681297623\n",
      "Cost after iteration 11600: 0.00012987724356529212\n",
      "Cost after iteration 11700: 0.00012832621640025392\n",
      "Cost after iteration 11800: 0.00012680601128808272\n",
      "Cost after iteration 11900: 0.0001253155891788625\n",
      "Cost after iteration 12000: 0.00012385395404887925\n",
      "Cost after iteration 12100: 0.00012242015111643326\n",
      "Cost after iteration 12200: 0.00012101326521946908\n",
      "Cost after iteration 12300: 0.00011963241934776458\n",
      "Cost after iteration 12400: 0.00011827677332290688\n",
      "Cost after iteration 12500: 0.00011694552261906952\n",
      "Cost after iteration 12600: 0.00011563789731759061\n",
      "Cost after iteration 12700: 0.00011435316118771164\n",
      "Cost after iteration 12800: 0.00011309061088518569\n",
      "Cost after iteration 12900: 0.00011184957525902709\n",
      "Cost after iteration 13000: 0.00011062941475541976\n",
      "Cost after iteration 13100: 0.00010942952090587651\n",
      "Cost after iteration 13200: 0.00010824931588428134\n",
      "Cost after iteration 13300: 0.00010708825211562406\n",
      "Cost after iteration 13400: 0.00010594581191618418\n",
      "Cost after iteration 13500: 0.00010482150714253896\n",
      "Cost after iteration 13600: 0.00010371487882487839\n",
      "Cost after iteration 13700: 0.00010262549675790192\n",
      "Cost after iteration 13800: 0.00010155295902183629\n",
      "Cost after iteration 13900: 0.00010049689140637895\n",
      "Cost after iteration 14000: 9.945694671123025e-05\n",
      "Cost after iteration 14100: 9.843280390068037e-05\n",
      "Cost after iteration 14200: 9.742416709338013e-05\n",
      "Cost after iteration 14300: 9.643076437516751e-05\n",
      "Cost after iteration 14400: 9.545234642989124e-05\n",
      "Cost after iteration 14500: 9.448868499197098e-05\n",
      "Cost after iteration 14600: 9.353957113273009e-05\n",
      "Cost after iteration 14700: 9.260481340183822e-05\n",
      "Cost after iteration 14800: 9.168423585246256e-05\n",
      "Cost after iteration 14900: 9.077767598539108e-05\n",
      "Cost after iteration 15000: 8.98849826517107e-05\n",
      "Cost after iteration 15100: 8.900601395581728e-05\n",
      "Cost after iteration 15200: 8.814063520032e-05\n",
      "Cost after iteration 15300: 8.728871691231773e-05\n",
      "Cost after iteration 15400: 8.645013298626874e-05\n",
      "Cost after iteration 15500: 8.56247589732452e-05\n",
      "Cost after iteration 15600: 8.481247054011215e-05\n",
      "Cost after iteration 15700: 8.401314211550724e-05\n",
      "Cost after iteration 15800: 8.32266457328477e-05\n",
      "Cost after iteration 15900: 8.245285007456502e-05\n",
      "Train accuracy: 100.00%\n",
      "Test accuracy: 86.00%\n"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "input_size = X_train.shape[0]\n",
    "hidden_size1 = 38\n",
    "hidden_size2 = 19\n",
    "output_size = 1\n",
    "num_iterations = 16000\n",
    "learning_rate = 0.52\n",
    "delay = 1\n",
    "\n",
    "\n",
    "W1, b1, W2, b2, W3, b3 = neural_network(X_train, y_train, input_size, hidden_size1, hidden_size2, output_size,\n",
    "                                        num_iterations, learning_rate)\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = predict(X_train, W1, b1, W2, b2, W3, b3)\n",
    "test_predictions = predict(X_test, W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = np.mean(train_predictions == y_train)\n",
    "test_accuracy = np.mean(test_predictions == y_test)\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d0733-1102-46d9-a260-f799bc4e023b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
